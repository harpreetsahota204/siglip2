{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use the SigLIP2 Model for Embeddings and Text Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\"quickstart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Zoo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "foz.register_zoo_model_source(\"https://github.com/harpreetsahota204/siglip2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a checkpoint\n",
    "\n",
    "\n",
    "- Size matters: Giant > So400m > Large > Base models for retrieval quality\n",
    "- Resolution matters: Higher resolutions (384, 512) consistently outperform lower ones (224, 256)\n",
    "\n",
    "#### Document vs. Natural Images:\n",
    "\n",
    "- For natural images (photos, etc.): Standard fixed-resolution models perform best\n",
    "- For document-like, text-heavy, or screen images: NaFlex variants perform better\n",
    "\n",
    "#### Specific Use Case Recommendations:\n",
    "\n",
    "- General image-text retrieval: Use larger models (So400m, Giant) with higher resolutions\n",
    "- Document/OCR/screen content: Use NaFlex variants, especially at lower resolutions\n",
    "- Multilingual applications: SigLIP 2 works well across languages\n",
    "\n",
    "You can choose from one of the available checkpoints:\n",
    "\n",
    "##### Base:\n",
    "\n",
    "- `google/siglip2-base-patch16-224`\n",
    "- `google/siglip2-base-patch16-256`  \n",
    "- `google/siglip2-base-patch16-384`\n",
    "- `google/siglip2-base-patch16-512`\n",
    "- `google/siglip2-base-patch32-256`\n",
    "- `google/siglip2-base-patch16-naflex`\n",
    "\n",
    "##### Large:\n",
    "\n",
    "- `google/siglip2-large-patch16-256`\n",
    "- `google/siglip2-large-patch16-384`\n",
    "- `google/siglip2-large-patch16-512`\n",
    "\n",
    "##### Giant:\n",
    "\n",
    "- `google/siglip2-giant-opt-patch16-256`\n",
    "- `google/siglip2-giant-opt-patch16-384`\n",
    "\n",
    "##### Shape optimized:\n",
    "\n",
    "So400m variants generally achieve higher retrieval performance (Recall@1) on benchmarks like COCO and Flickr compared to the Base and Large models.\n",
    "\n",
    "- `google/siglip2-so400m-patch14-224`\n",
    "- `google/siglip2-so400m-patch14-384`\n",
    "- `google/siglip2-so400m-patch16-256`\n",
    "- `google/siglip2-so400m-patch16-384`\n",
    "- `google/siglip2-so400m-patch16-512`\n",
    "- `google/siglip2-so400m-patch16-naflex`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foz.download_zoo_model(\n",
    "    \"https://github.com/harpreetsahota204/siglip2\",\n",
    "    model_name=\"google/siglip2-so400m-patch16-naflex\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "model = foz.load_zoo_model(\n",
    "    \"google/siglip2-so400m-patch16-naflex\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_embeddings(\n",
    "    model=model,\n",
    "    embeddings_field=\"siglip2_embeddings\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute visualization of embeddings\n",
    "\n",
    "Note requires that `umap-learn` is installed. Currently, `umap-learn` only supports `numpy<=2.1.0`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization...\n",
      "UMAP( verbose=True)\n",
      "Mon Apr 21 13:27:55 2025 Construct fuzzy simplicial set\n",
      "Mon Apr 21 13:27:55 2025 Finding Nearest Neighbors\n",
      "Mon Apr 21 13:27:58 2025 Finished Nearest Neighbor Search\n",
      "Mon Apr 21 13:28:00 2025 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1866cf8a097f4f5da038b1726fc6d97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Mon Apr 21 13:28:02 2025 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=\"siglip2_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key=\"siglip2_viz\",\n",
    "    num_dims=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a similarity index for natural language search\n",
    "\n",
    "You can [visit the docs](https://docs.voxel51.com/api/fiftyone.brain.html?highlight=compute_similarity#fiftyone.brain.compute_similarity) for more information on similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings...\n",
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [6.2s elapsed, 0s remaining, 54.6 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "text_img_index = fob.compute_similarity(\n",
    "    dataset,\n",
    "    model=\"google/siglip2-so400m-patch16-naflex\", #or just pass in the already instantiated model\n",
    "    brain_key=\"siglip2_sim\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that we can support text search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(text_img_index.config.supports_prompts)  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    }
   ],
   "source": [
    "sims = text_img_index.sort_by_similarity(\n",
    "    \"a dude on a horse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset:     quickstart\n",
       "Media type:  image\n",
       "Num samples: 200\n",
       "Sample fields:\n",
       "    id:                 fiftyone.core.fields.ObjectIdField\n",
       "    filepath:           fiftyone.core.fields.StringField\n",
       "    tags:               fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:           fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    created_at:         fiftyone.core.fields.DateTimeField\n",
       "    last_modified_at:   fiftyone.core.fields.DateTimeField\n",
       "    ground_truth:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    uniqueness:         fiftyone.core.fields.FloatField\n",
       "    predictions:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    siglip2_embeddings: fiftyone.core.fields.VectorField\n",
       "View stages:\n",
       "    1. Select(sample_ids=['68068dcdb62578c64484be40', '68068dccb62578c64484bdd4', '68068dceb62578c64484be5e', ...], ordered=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select your Dataset from the dropdown menu, open the embeddings panel by clicking the `+` next to the Samples viewer, and select the embeddings you want to display by selecting from the dropdown menu in the embeddings panel.\n",
    "\n",
    "To search via natural language in the App click the `ðŸ”Ž` button and type in your query. The most similar samples to the query will be shown in decreasing order of similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
